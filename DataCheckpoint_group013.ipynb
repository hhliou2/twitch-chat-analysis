{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Jianan Liu\n",
    "- Casey Lee\n",
    "- Mark Bussard\n",
    "- Aryan Ziyar\n",
    "- Hasan Liou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By merit of the existence of subcultures on Twitch, can we reliably identify a Twitch streamer's channel by performing machine learning analyses on their respective chatlogs? Furthermore, does Twitch emote usage vary from streamer to streamer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your dataset information here*\n",
    "\n",
    "- Dataset Name: Twitch.tv Chat Log Data\n",
    "- Dataset Link: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VE0IVQ\n",
    "- Number of Observations: 10113500 rows, 9 columns\n",
    "\n",
    "The chatlog data contains data from 2,162 Videos on Demand (VODs) from 52 streamers. For simplicity of calculation, we will subsample chats from streamers in the following set: loltyler1, xqcow, imaqtpie, Ninja, and TSM_Myth.\n",
    "\n",
    "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import urllib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here.\n",
    "1. We called in a dataset for analysis from the Harvard Dataverse database. \n",
    "\n",
    "2. We wanted to filter out the emotes that were channel specific, using a 3rd party Twitch Emote API. We were able to narrow down the list of emotes to non-channel specific emotes (also known as global emotes) and we filtered out non-global emotes and placed the new emote list in a new Series in our Dataframe\n",
    "\n",
    "3. After that, we further refined each text field by creating a new Series for text fragments only and removed all emotes (however, we decided to keep Unicode emojis) We also removed urls using regex and we also removed all identifying information pertaining to the channel (ie if message from the chat had the streamer's name, we removed that name from the text only Series)\n",
    "\n",
    "4. Finally, we created one more new series, known as created_at, which we used to parse into day time analysis over a certain period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in subsamples of streamers\n",
    "xqc = pd.read_pickle('D:/Documents/HW/COGS108/xqcow.pkl').sample(10000)\n",
    "tim = pd.read_pickle('D:/Documents/HW/COGS108/timthetatman.pkl').sample(10000)\n",
    "tyler1 = pd.read_pickle('D:/Documents/HW/COGS108/loltyler1.pkl').sample(10000)\n",
    "\n",
    "qtpie = pd.read_pickle('D:/Documents/HW/COGS108/imaqtpie.pkl').sample(10000)\n",
    "myth = pd.read_pickle('D:/Documents/HW/COGS108/tsm_myth.pkl').sample(10000)\n",
    "ninja = pd.read_pickle('D:/Documents/HW/COGS108/ninja.pkl').sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of global emotes\n",
    "\n",
    "with urllib.request.urlopen(\"https://api.twitchemotes.com/api/v4/channels/0\") as url:\n",
    "    requested = json.loads(url.read().decode())\n",
    "emote_list = requested['emotes']\n",
    "id_list = [int(x['id']) for x in emote_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emote function (take only one of each global emote, ignore rest)\n",
    "def make_emote_list(x):\n",
    "    lst = []\n",
    "    # For each fragment\n",
    "    for fragment in x:\n",
    "        # Check that we have an emote fragment, and that it is a global emote\n",
    "        if ('emoticon_id' in fragment.keys()) and (int(fragment['emoticon_id']) in id_list):\n",
    "            # Add emote\n",
    "            lst.append(int(fragment['emoticon_id']))\n",
    "    \n",
    "    # Save only unique emotes\n",
    "    return list(set(lst))\n",
    "\n",
    "# Apply function to dataset fragments\n",
    "xqc['emotes'] = xqc.fragments.apply(make_emote_list)\n",
    "tim['emotes'] = tim.fragments.apply(make_emote_list)\n",
    "tyler1['emotes'] = tyler1.fragments.apply(make_emote_list)\n",
    "\n",
    "qtpie['emotes'] = qtpie.fragments.apply(make_emote_list)\n",
    "myth['emotes'] = myth.fragments.apply(make_emote_list)\n",
    "ninja['emotes'] = ninja.fragments.apply(make_emote_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only text (no emotes)\n",
    "def get_text_only(x):\n",
    "    # Combines all elements in dict with \"text\" key\n",
    "    text_str = \" \".join([y['text'] for y in x if 'text' in y.keys()])\n",
    "    \n",
    "    # Removes trailing whitespaces and uppercase for analysis\n",
    "    return text_str.lower().strip()\n",
    "\n",
    "xqc['text_only'] = xqc.fragments.apply(get_text_only)\n",
    "tim['text_only'] = tim.fragments.apply(get_text_only)\n",
    "tyler1['text_only'] = tyler1.fragments.apply(get_text_only)\n",
    "\n",
    "qtpie['text_only'] = qtpie.fragments.apply(get_text_only)\n",
    "myth['text_only'] = myth.fragments.apply(get_text_only)\n",
    "ninja['text_only'] = ninja.fragments.apply(get_text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex out URLs\n",
    "regex_str = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "xqc['text_only'] = xqc.text_only.str.replace(regex_str, '', regex=True)\n",
    "tim['text_only'] = tim.text_only.str.replace(regex_str, '', regex=True)\n",
    "tyler1['text_only'] = tyler1.text_only.str.replace(regex_str, '', regex=True)\n",
    "\n",
    "qtpie['text_only'] = qtpie.text_only.str.replace(regex_str, '', regex=True)\n",
    "myth['text_only'] = myth.text_only.str.replace(regex_str, '', regex=True)\n",
    "ninja['text_only'] = ninja.text_only.str.replace(regex_str, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove identifying information\n",
    "xqc_identifying_info = '|'.join(['@xqcow', 'xqcow', 'xqc'])\n",
    "tim_identifying_info = '|'.join(['@timthetatman', 'timthetatman', 'tatman', 'tim'])\n",
    "tyler1_identifying_info = '|'.join(['@loltyler1', 'loltyler1', 'tyler1', 'tyler'])\n",
    "\n",
    "qtpie_identifying_info = '|'.join(['@imaqtpie', 'imaqtpie', 'qtpie', 'qt'])\n",
    "myth_identifying_info = '|'.join(['@myth', 'myth'])\n",
    "ninja_identifying_info = '|'.join(['@ninja', 'ninja'])\n",
    "\n",
    "xqc['text_only'] = xqc.text_only.str.replace(xqc_identifying_info, '')\n",
    "tim['text_only'] = tim.text_only.str.replace(tim_identifying_info, '')\n",
    "tyler1['text_only'] = tyler1.text_only.str.replace(tyler1_identifying_info, '')\n",
    "\n",
    "qtpie['text_only'] = qtpie.text_only.str.replace(qtpie_identifying_info, '')\n",
    "myth['text_only'] = myth.text_only.str.replace(myth_identifying_info, '')\n",
    "ninja['text_only'] = ninja.text_only.str.replace(ninja_identifying_info, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime parsing\n",
    "xqc['created_date'] = pd.to_datetime(xqc['created_at'])\n",
    "tim['created_date'] = pd.to_datetime(tim['created_at'])\n",
    "tyler1['created_date'] = pd.to_datetime(tyler1['created_at'])\n",
    "\n",
    "qtpie['created_date'] = pd.to_datetime(qtpie['created_at'])\n",
    "myth['created_date'] = pd.to_datetime(myth['created_at'])\n",
    "ninja['created_date'] = pd.to_datetime(ninja['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xqc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6719b0b064dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxqc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'xqc' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
