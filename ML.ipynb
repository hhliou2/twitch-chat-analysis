{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed68fd73-0954-49b8-b98c-a512bf53c88e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db6ea5c8-3665-452c-b1f3-54493097a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import twitch\n",
    "import urllib\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a751e-76dc-46c2-a279-0d3bcaf6cb2c",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8075f04b-c0e2-461c-b5c7-1839da3d3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in subsamples of streamers\n",
    "xqc = pd.read_pickle('Data/xqcow.pkl').sample(10000)\n",
    "tim = pd.read_pickle('Data/timthetatman.pkl').sample(10000)\n",
    "tyler1 = pd.read_pickle('Data/loltyler1.pkl').sample(10000)\n",
    "\n",
    "qtpie = pd.read_pickle('Data/imaqtpie.pkl').sample(10000)\n",
    "myth = pd.read_pickle('Data/tsm_myth.pkl').sample(10000)\n",
    "ninja = pd.read_pickle('Data/ninja.pkl').sample(10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ec010f85-23b4-4964-b894-c98f9c0ccf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Twitch API keys (stored locally)\n",
    "with open('api_keys.json') as f:\n",
    "    keys = json.load(f)\n",
    "\n",
    "# Get channel ID's from our streamers\n",
    "helix = twitch.Helix(keys['client_id'], keys['client_secret'])\n",
    "streamers = ['xqcow', 'timthetatman', 'loltyler1', 'imaqtpie', 'myth', 'ninja']\n",
    "\n",
    "# Get all streamer ID's\n",
    "streamer_ids = [helix.user(s).data['id'] for s in streamers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c1a1b2c0-8070-4465-b6cb-520b3150c676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitch emote ID subset: [12, 17, 10, 864205, 47]\n",
      "BTTV Emote subset: [':tf:', 'cigrip', 'datsauce', 'foreveralone', 'gaben']\n"
     ]
    }
   ],
   "source": [
    "# Create list of Twitch official global emotes\n",
    "with urllib.request.urlopen(\"https://api.twitchemotes.com/api/v4/channels/0\") as url:\n",
    "    requested = json.loads(url.read().decode())\n",
    "emote_list = requested['emotes']\n",
    "\n",
    "# match id's to emotes\n",
    "id_emote = {}\n",
    "for emote_dict in emote_list:\n",
    "    id_emote[emote_dict['id']] = emote_dict['code']\n",
    "id_list = [int(x['id']) for x in emote_list]\n",
    "\n",
    "# Get global and channel specific BTTV emotes to throw out of our analysis\n",
    "bttv_emotes = []\n",
    "with urllib.request.urlopen(\"https://api.betterttv.net/3/cached/emotes/global\") as url:\n",
    "    requested = json.loads(url.read().decode())\n",
    "bttv_emotes = [emote['code'].lower() for emote in requested]\n",
    "\n",
    "for s in streamer_ids:\n",
    "    with urllib.request.urlopen(\"https://api.betterttv.net/3/cached/users/twitch/\" + s) as url:\n",
    "        requested = json.loads(url.read().decode())\n",
    "        streamer_emotes = [emote['code'].lower() for emote in requested['channelEmotes']]\n",
    "    bttv_emotes += streamer_emotes\n",
    "\n",
    "print(\"Twitch emote ID subset: \" + str(id_list[:5]))\n",
    "print(\"BTTV Emote subset: \" + str(bttv_emotes[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "43d46df9-17b6-427e-b342-5f1fa9a62668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6]\n"
     ]
    }
   ],
   "source": [
    "# Emote function (take only one of each global emote, ignore rest)\n",
    "def make_emote_list(x):\n",
    "    lst = []\n",
    "    # For each fragment\n",
    "    for fragment in x:\n",
    "        # Check that we have an emote fragment, and that it is a global emote\n",
    "        if ('emoticon_id' in fragment.keys()) and (int(fragment['emoticon_id']) in id_list):\n",
    "            # Add emote\n",
    "            lst.append(int(fragment['emoticon_id']))\n",
    "    \n",
    "    # Save only unique emotes\n",
    "    return list(set(lst))\n",
    "\n",
    "# Apply function to dataset fragments\n",
    "xqc['emotes'] = xqc.fragments.apply(make_emote_list)\n",
    "tim['emotes'] = tim.fragments.apply(make_emote_list)\n",
    "tyler1['emotes'] = tyler1.fragments.apply(make_emote_list)\n",
    "\n",
    "qtpie['emotes'] = qtpie.fragments.apply(make_emote_list)\n",
    "myth['emotes'] = myth.fragments.apply(make_emote_list)\n",
    "ninja['emotes'] = ninja.fragments.apply(make_emote_list)\n",
    "\n",
    "# Example output\n",
    "test_fragments = [{'emoticon_id': '4'}, {'text': 'sample text'}, {'emoticon_id': '6'}]\n",
    "print(make_emote_list(test_fragments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7fc542ee-474d-44ce-a709-dd76ed49023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text\n"
     ]
    }
   ],
   "source": [
    "# Take only text (no emotes)\n",
    "def get_text_only(x):\n",
    "    # Combines all elements in dict with \"text\" key\n",
    "    text_str = \" \".join([y['text'] for y in x if 'text' in y.keys()])\n",
    "    \n",
    "    # Removes trailing whitespaces and uppercase for analysis\n",
    "    return text_str.lower().strip()\n",
    "\n",
    "xqc['text_only'] = xqc.fragments.apply(get_text_only)\n",
    "tim['text_only'] = tim.fragments.apply(get_text_only)\n",
    "tyler1['text_only'] = tyler1.fragments.apply(get_text_only)\n",
    "\n",
    "qtpie['text_only'] = qtpie.fragments.apply(get_text_only)\n",
    "myth['text_only'] = myth.fragments.apply(get_text_only)\n",
    "ninja['text_only'] = ninja.fragments.apply(get_text_only)\n",
    "\n",
    "# Example output\n",
    "print(get_text_only(test_fragments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8e61b720-4bb2-44be-81ee-776f7075fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check out  for our project repo, or use \n"
     ]
    }
   ],
   "source": [
    "# Regex out URLs\n",
    "regex_str = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "xqc['text_only'] = xqc.text_only.str.replace(regex_str, '', regex=True)\n",
    "tim['text_only'] = tim.text_only.str.replace(regex_str, '', regex=True)\n",
    "tyler1['text_only'] = tyler1.text_only.str.replace(regex_str, '', regex=True)\n",
    "\n",
    "qtpie['text_only'] = qtpie.text_only.str.replace(regex_str, '', regex=True)\n",
    "myth['text_only'] = myth.text_only.str.replace(regex_str, '', regex=True)\n",
    "ninja['text_only'] = ninja.text_only.str.replace(regex_str, '', regex=True)\n",
    "\n",
    "# Example output\n",
    "sample_text = 'check out https://github.com/COGS108/group013_sp21 for our project repo, or use !link'\n",
    "sample_text = re.sub(regex_str, '', sample_text)\n",
    "\n",
    "# Regex out commands\n",
    "regex_str = r'\\!\\w+'\n",
    "\n",
    "xqc['text_only'] = xqc.text_only.str.replace(regex_str, '', regex=True)\n",
    "tim['text_only'] = tim.text_only.str.replace(regex_str, '', regex=True)\n",
    "tyler1['text_only'] = tyler1.text_only.str.replace(regex_str, '', regex=True)\n",
    "\n",
    "qtpie['text_only'] = qtpie.text_only.str.replace(regex_str, '', regex=True)\n",
    "myth['text_only'] = myth.text_only.str.replace(regex_str, '', regex=True)\n",
    "ninja['text_only'] = ninja.text_only.str.replace(regex_str, '', regex=True)\n",
    "\n",
    "# Example Output\n",
    "sample_text = re.sub(regex_str, '', sample_text)\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "269423e5-63c3-44ba-b8b4-bdd2b625e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love you  please respond'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove identifying information\n",
    "xqc_identifying_info = '|'.join(['@xqcow', 'xqcow', 'xqc'])\n",
    "tim_identifying_info = '|'.join(['@timthetatman', 'timthetatman', 'tatman', 'tim'])\n",
    "tyler1_identifying_info = '|'.join(['@loltyler1', 'loltyler1', 'tyler1', 'tyler'])\n",
    "\n",
    "qtpie_identifying_info = '|'.join(['@imaqtpie', 'imaqtpie', 'qtpie', 'qt'])\n",
    "myth_identifying_info = '|'.join(['@tsm_myth', '@myth', 'tsm_myth', 'myth'])\n",
    "ninja_identifying_info = '|'.join(['@ninja', 'ninja'])\n",
    "\n",
    "xqc['text_only'] = xqc.text_only.str.replace(xqc_identifying_info, '', regex=True)\n",
    "tim['text_only'] = tim.text_only.str.replace(tim_identifying_info, '', regex=True)\n",
    "tyler1['text_only'] = tyler1.text_only.str.replace(tyler1_identifying_info, '', regex=True)\n",
    "\n",
    "qtpie['text_only'] = qtpie.text_only.str.replace(qtpie_identifying_info, '', regex=True)\n",
    "myth['text_only'] = myth.text_only.str.replace(myth_identifying_info, '', regex=True)\n",
    "ninja['text_only'] = ninja.text_only.str.replace(ninja_identifying_info, '', regex=True)\n",
    "\n",
    "# Example Output\n",
    "re.sub(xqc_identifying_info, '', 'i love you @xqcow please respond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "675ff212-5852-44f5-af90-f0e8f5c4ea5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh man i need help    please   me for help        karappa'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only alphanumeric characters\n",
    "regex_str = r'\\W'\n",
    "xqc['text_only'] = xqc.text_only.str.replace(regex_str, ' ', regex=True)\n",
    "tim['text_only'] = tim.text_only.str.replace(regex_str, ' ', regex=True)\n",
    "tyler1['text_only'] = tyler1.text_only.str.replace(regex_str, ' ', regex=True)\n",
    "\n",
    "qtpie['text_only'] = qtpie.text_only.str.replace(regex_str, ' ', regex=True)\n",
    "myth['text_only'] = myth.text_only.str.replace(regex_str, ' ', regex=True)\n",
    "ninja['text_only'] = ninja.text_only.str.replace(regex_str, ' ', regex=True)\n",
    "\n",
    "# Example Output\n",
    "example = re.sub(regex_str, ' ', 'oh man i need help!!! please @ me for help#!@!#!# karappa')\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "edb89d48-aabf-4db9-be8f-1743f7ef584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbook/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oh man need help please help'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords + bttv emotes + extra spaces\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english') + bttv_emotes\n",
    "\n",
    "xqc['text_only'] = xqc['text_only'].apply(lambda x: ' '.join([item for item in x.split() if item not in sw]))\n",
    "tim['text_only'] = tim['text_only'].apply(lambda x: ' '.join([item for item in x.split() if item not in sw]))\n",
    "tyler1['text_only'] = tyler1['text_only'].apply(lambda x: ' '.join([item for item in x.split() if item not in sw]))\n",
    "\n",
    "qtpie['text_only'] = qtpie['text_only'].apply(lambda x: ' '.join([item for item in x.split() if item not in sw]))\n",
    "myth['text_only'] = myth['text_only'].apply(lambda x: ' '.join([item for item in x.split() if item not in sw]))\n",
    "ninja['text_only'] = ninja['text_only'].apply(lambda x: ' '.join([item for item in x.split() if item not in sw]))\n",
    "\n",
    "example = ' '.join([item for item in example.split() if item not in sw])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "325fba93-17ba-41e5-b106-df0e67a5d00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xqcOW: 0\n",
      "TimTheTatMan: 1\n",
      "loltyler1: 14\n",
      "imaqtpie: 5\n",
      "TSM_Myth: 1\n",
      "Ninja: 0\n"
     ]
    }
   ],
   "source": [
    "print('xqcOW: ' + str(len(xqc[xqc['body'].str.contains(\"alpha\")])))\n",
    "print('TimTheTatMan: ' + str(len(tim[tim['body'].str.contains(\"alpha\")])))\n",
    "print('loltyler1: ' + str(len(tyler1[tyler1['body'].str.contains(\"alpha\")])))\n",
    "\n",
    "print('imaqtpie: ' + str(len(qtpie[qtpie['body'].str.contains(\"alpha\")])))\n",
    "print('TSM_Myth: ' + str(len(myth[myth['body'].str.contains(\"alpha\")])))\n",
    "print('Ninja: ' + str(len(ninja[ninja['body'].str.contains(\"alpha\")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1173ddf4-1189-43b0-8925-9cf713d1c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, analyzer='word', max_features=2000, tokenizer=word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cd53e8c0-0423-4c17-a28f-ef5991ef6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty string ('') from text_only for each of the datasets\n",
    "xqc_final = pd.Series([i for i in xqc['text_only'] if i]) \n",
    "tim_final = pd.Series([i for i in tim['text_only'] if i])\n",
    "tyler1_final = pd.Series([i for i in tyler1['text_only'] if i])\n",
    "qtpie_final = pd.Series([i for i in qtpie['text_only'] if i])\n",
    "myth_final = pd.Series([i for i in myth['text_only'] if i])\n",
    "ninja_final = pd.Series([i for i in ninja['text_only'] if i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648133f-8420-4208-92a0-f4949f9fb4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'text_only' columns from each of the streamers into one dataset with labels\n",
    "xqc_frame = {'Chat Text': xqc_final}\n",
    "xqc_result = pd.DataFrame(xqc_frame)\n",
    "xqc_result['label'] = 'xqc'\n",
    "\n",
    "tim_frame = {'Chat Text': tim_final}\n",
    "tim_result = pd.DataFrame(tim_frame)\n",
    "tim_result['label'] = 'tim'\n",
    "\n",
    "\n",
    "tyler1_frame = {'Chat Text': tyler1_final}\n",
    "tyler1_result = pd.DataFrame(tyler1_frame)\n",
    "tyler1_result['label'] = 'tyler1'\n",
    "\n",
    "\n",
    "qtpie_frame = {'Chat Text': qtpie_final}\n",
    "qtpie_result = pd.DataFrame(qtpie_frame)\n",
    "qtpie_result['label'] = 'qtpie'\n",
    "\n",
    "\n",
    "myth_frame = {'Chat Text': myth_final}\n",
    "myth_result = pd.DataFrame(myth_frame)\n",
    "myth_result['label'] = 'myth'\n",
    "\n",
    "\n",
    "ninja_frame = {'Chat Text': ninja_final}\n",
    "ninja_result = pd.DataFrame(ninja_frame)\n",
    "ninja_result['label'] = 'ninja'\n",
    "\n",
    "\n",
    "frames = [xqc_result, tim_result, tyler1_result, qtpie_result, myth_result, ninja_result]\n",
    "text_frame = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0ef7f-b92d-4a51-bf06-c9d744f6baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "text_frame = text_frame.sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "\n",
    "tfidf_X = tfidf.fit_transform(text_frame['Chat Text']).toarray()\n",
    "\n",
    "tfidf_Y = text_frame['label'].values\n",
    "text_frame = text_frame.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2041d-61d1-4d44-bb93-e7533f35df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_tfidf_X, test_tfidf_X, train_tfidf_Y, test_tfidf_Y = train_test_split(tfidf_X, tfidf_Y, test_size=0.2, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "55e24e13-ec37-4338-a81b-339668d5ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SVM(X, y, kernel='linear'):\n",
    "# YOUR CODE HERE\n",
    "    clf = SVC(kernel=kernel)\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4a85fa2e-fbf4-4724-8e8c-72906ad8aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tfidf_clf = train_SVM(train_tfidf_X, train_tfidf_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51613c-3c52-4bd2-bdf4-6c3b6975d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions based on the model\n",
    "prediction_train_tfidf_Y = tfidf_clf.predict(train_tfidf_X)\n",
    "prediction_test_tfidf_Y = tfidf_clf.predict(test_tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5010f6-7763-41dc-bce7-24774666ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the model with training data\n",
    "print(classification_report(train_tfidf_Y, prediction_train_tfidf_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682f4b5-3366-4993-b2dd-15d76d6b95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assesss the model with test data\n",
    "print(classification_report(test_tfidf_Y, prediction_test_tfidf_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989da35-73d7-48ed-91e8-b5a66522c523",
   "metadata": {},
   "source": [
    "# Emote TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd787ab4-ab2f-47cf-80cc-3937f04a1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xqc_emote_final = pd.Series([i for i in xqc['emotes'] if i]) \n",
    "tim_emote_final = pd.Series([i for i in tim['emotes'] if i])\n",
    "tyler1_emote_final = pd.Series([i for i in tyler1['emotes'] if i])\n",
    "qtpie_emote_final = pd.Series([i for i in qtpie['emotes'] if i])\n",
    "myth_emote_final = pd.Series([i for i in myth['emotes'] if i])\n",
    "ninja_emote_final = pd.Series([i for i in ninja['emotes'] if i])\n",
    "\n",
    "# Combine the 'text_only' columns from each of the streamers into one dataset with labels\n",
    "xqc_emote_frame = {'Emote': xqc_emote_final}\n",
    "xqc_result = pd.DataFrame(xqc_frame)\n",
    "xqc_result['label'] = 'xqc'\n",
    "\n",
    "tim_frame = {'Emote': tim_emote_final}\n",
    "tim_result = pd.DataFrame(tim_frame)\n",
    "tim_result['label'] = 'tim'\n",
    "\n",
    "\n",
    "tyler1_frame = {'Emote': tyler1_emote_final}\n",
    "tyler1_result = pd.DataFrame(tyler1_frame)\n",
    "tyler1_result['label'] = 'tyler1'\n",
    "\n",
    "\n",
    "qtpie_frame = {'Emote': qtpie_emote_final}\n",
    "qtpie_result = pd.DataFrame(qtpie_frame)\n",
    "qtpie_result['label'] = 'qtpie'\n",
    "\n",
    "\n",
    "myth_frame = {'Emote': myth_emote_final}\n",
    "myth_result = pd.DataFrame(myth_frame)\n",
    "myth_result['label'] = 'myth'\n",
    "\n",
    "\n",
    "ninja_frame = {'Emote': ninja_emote_final}\n",
    "ninja_result = pd.DataFrame(ninja_frame)\n",
    "ninja_result['label'] = 'ninja'\n",
    "\n",
    "\n",
    "emote_frames = [xqc_result, tim_result, tyler1_result, qtpie_result, myth_result, ninja_result]\n",
    "emote_frame = pd.concat(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31200a82-15cf-412e-9d8d-bfc6063673e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6189982f-4988-490a-8b67-5194e57baeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
